---
title: "class09_mini_project.Rmd"
author: "Sara Herrera (PID:A59011948)"
date: "10/27/2021"
output: pdf_document
---

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

# Examine the input data
#wisc.df

# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]

# Create diagnosis vector for later 
diagnosis <- as.factor(wisc.df$diagnosis)
#diagnosis
```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
colnames(wisc.df)
grep("_mean", colnames(wisc.df))
```
```{r}
length(grep("_mean", colnames(wisc.df)))
```

# Principal Component Analysis

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale=TRUE )
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

**44.27%**

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

**PC1, PC2, and PC3 (by looking at the cumulative proportion)**

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

**7 PCs (PC1-PC7)**

# Interpretation of PCA results

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

**This plot is difficult to understand because there are too many things.**

```{r}
# Scatter plot observations by components 1 and 2, PC1 vs PC2
plot( wisc.pr$x , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")

# Can also use plot(wisc.pr$x[,1:2], col=diagnosis)
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

**Component 2 explains more of the variance than component 3, but, essentially, these plots are able to separate malignant (red) from benign (black) samples.**

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,c(1,3)], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

# Variance explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation[,1]
```
**For the first principal component, the component of the loading vector for the feature concave.points_mean is -0.26085376.**

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
#summary(wisc.pr)
```
**The number of principal components required to explain 80% of the variance is 5 (PC1-PC5).**

# Hierarchical clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)

# Calculate the Euclidean distances
data.dist <- dist(data.scaled)

# Create hierarchical clustering
wisc.hclust <- hclust(data.dist, method="complete")
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

**Height at which the clustering model has 4 clusters is 19.**

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

# Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
#wisc.hclust.clusters
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```
**I think cutting into 4 clusters gives a better cluster vs diagnoses match because it's the first time the 2 diagnoses split clearly between malignant and benign.**

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
wisc.hclust1 <- hclust(data.dist, method="single")
wisc.hclust1
plot(wisc.hclust1)
```
```{r}
wisc.hclust1 <- hclust(data.dist, method="average")
wisc.hclust1
plot(wisc.hclust1)
```

```{r}
wisc.hclust1 <- hclust(data.dist, method="ward.D2")
wisc.hclust1
plot(wisc.hclust1)
```

**The method that gives my favorite result is using method="ward.D2" because I can start to see the difference between the 2 diagnoses/clusters.**

# Clustering on PCA results
**I will use 7 PCs and `hclust()` and `dist()` as an input to describe at least 90% of the variability in the data**
```{r}
wisc.pr.hclust <- hclust( dist(wisc.pr$x[,1:7]), method="ward.D2")
```
```{r}
plot(wisc.pr.hclust)
abline(h=80, col="red")
```

**Let's find our cluster membership vector by cutting this tree into k=2 groups.**
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

**We can do a cross-table to see how many B's or M's are in each group/cluster**
```{r}
table(grps, diagnosis)
```
**With this plot we can differentiate True Negative = 329, True Positive = 188, False Positive = 28, and False Negative = 24**
**"Accuracy", essentially how many did we get correct:**
```{r}
(188+329) / nrow(wisc.data)
```
**"Sensitivity", the true ill we got correct:**
```{r}
(188 / (188 + 24))
```
**"Specificity":**
```{r}
(329 / (329+24))
```

# 7. Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
#npc
```

**Now add these new samples to our PCA plot**
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], labels=c(1,2), col="white")
```

















